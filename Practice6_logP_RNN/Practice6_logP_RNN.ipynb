{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f829a2c4830>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from torchtext.data import Field, RawField, NestedField, TabularDataset, BucketIterator\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from utils import *\n",
    "from visual_tool import *\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading & Vocabulary Building\n",
    "\n",
    "torchtext character embedding ref : http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/  \n",
    "torchtext ref : http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/  \n",
    "NestedField ref : https://github.com/pytorch/text/blob/master/test/sequence_tagging.py  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILE = Field(sequential=True, tokenize=list, lower=False)\n",
    "CHAR_SMILE = NestedField(SMILE) #, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "LOGP = RawField(preprocessing=float)\n",
    "LENGTH = RawField(preprocessing=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [('smile', CHAR_SMILE),\n",
    "              ('logp', LOGP),\n",
    "              ('mr', None), \n",
    "              ('tpsa', None), \n",
    "              ('length', LENGTH)]\n",
    "\n",
    "train_dataset, val_dataset = TabularDataset.splits(\n",
    "    path=\"../Data/\",\n",
    "    train=\"train000000.csv\", validation=\"val000000.csv\",\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=datafields\n",
    ")\n",
    "\n",
    "CHAR_SMILE.build_vocab(train_dataset.smile, val_dataset.smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# === Integer to Char === #\n",
      "['<unk>', '<pad>', 'C', 'c', '(', ')', '1', 'O', '@', 'N', '2', '=', '[', ']', 'H', 'n', 'F', '3', 'S', 'l', '-', 's', 'B', 'r', '#', 'o', '4', '+', '/', '\\\\', 'I', '5', 'i', 'P', '6']\n",
      "# === Char to Integer === #\n",
      "defaultdict(<function _default_unk_index at 0x7f825b8b5d90>, {'<unk>': 0, '<pad>': 1, 'C': 2, 'c': 3, '(': 4, ')': 5, '1': 6, 'O': 7, '@': 8, 'N': 9, '2': 10, '=': 11, '[': 12, ']': 13, 'H': 14, 'n': 15, 'F': 16, '3': 17, 'S': 18, 'l': 19, '-': 20, 's': 21, 'B': 22, 'r': 23, '#': 24, 'o': 25, '4': 26, '+': 27, '/': 28, '\\\\': 29, 'I': 30, '5': 31, 'i': 32, 'P': 33, '6': 34})\n"
     ]
    }
   ],
   "source": [
    "print(\"# === Integer to Char === #\")\n",
    "print(CHAR_SMILE.vocab.itos)\n",
    "print(\"# === Char to Integer === #\")\n",
    "print(CHAR_SMILE.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "             \n",
    "    def forward(self, x):\n",
    "        origin_shape = x.shape\n",
    "        x = x.view(-1, origin_shape[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(origin_shape)\n",
    "        return x\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_dim, batch_size, output_dim=1,\n",
    "                 num_layers=2, dropout=0.1, \n",
    "                 bidirectional=True, emb_train=True, skip_connection=True, \n",
    "                 cell_type='lstm', norm_type='ln'):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.highway_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.emb_train = emb_train\n",
    "        self.skip_connection = skip_connection\n",
    "        \n",
    "        self.cell_type = cell_type\n",
    "        self.norm_type = norm_type\n",
    "        \n",
    "        # Define embedding layer\n",
    "        self.embedding = self.create_emb_layer(self.vocab_size, self.emb_train)\n",
    "        self.feeding_fc = nn.Linear(self.vocab_size, self.highway_dim)\n",
    "        \n",
    "        # Define Normalization layer\n",
    "        if self.norm_type == 'bn':\n",
    "            self.norm = BN1d(self.highway_dim)\n",
    "        elif self.norm_type == 'ln':\n",
    "            self.norm = nn.LayerNorm(self.highway_dim)\n",
    "        else:\n",
    "            self.norm = nn.Sequential()\n",
    "        \n",
    "        # Define the Recurrent layer\n",
    "        self.lstms = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            if self.cell_type == 'lstm':\n",
    "                self.lstms.append(nn.LSTM(self.highway_dim, self.hidden_dim, \n",
    "                                       self.num_layers, dropout=self.dropout, \n",
    "                                       bidirectional=self.bidirectional))\n",
    "            elif self.cell_type == 'gru':\n",
    "                self.lstms.append(nn.GRU(self.highway_dim, self.hidden_dim, \n",
    "                                         self.num_layers, dropout=self.dropout, \n",
    "                                         bidirectional=self.bidirectional))\n",
    "        # Define the output layer\n",
    "        self.fc1 = nn.Linear(self.highway_dim, self.highway_dim // 2)\n",
    "        self.fc2 = nn.Linear(self.highway_dim // 2, self.highway_dim )\n",
    "        self.fc3 = nn.Linear(self.highway_dim, self.output_dim)\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.highway_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.highway_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "\n",
    "        # shape of input: [seq_length, batch_size, 1]\n",
    "        # shape of emb_input: [seq_length, batch_size, 1, vocab_size]\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "\n",
    "        lstm_input = self.feeding_fc(self.embedding(input).squeeze())\n",
    "         \n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            lstm_out, self.hidden = lstm(lstm_input.view(len(input), self.batch_size, -1))\n",
    "            lstm_input = lstm_out + lstm_input if self.skip_connection else lstm_out # skip connection\n",
    "            print(lstm_input.shape)\n",
    "            lstm_input = self.norm(lstm_input)\n",
    "            \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        x = self.dropout(self.relu(self.fc1(lstm_out[-1].view(self.batch_size, -1))))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x.view(-1)\n",
    "    \n",
    "    def create_emb_layer(self, vocab_size, emb_train):\n",
    "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "        for i in range(vocab_size):\n",
    "            weight_matrix[i][i] = 1\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "\n",
    "        if not emb_train:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_iter, optimizer, criterion, args, **kwargs):\n",
    "        \n",
    "    epoch_train_loss = 0\n",
    "    list_train_loss = list()\n",
    "    cnt_iter = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        if batch.smile.shape[0] != args.batch_size:\n",
    "            continue\n",
    "\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        optimizier.zero_grad()\n",
    "\n",
    "        input_smile = batch.smile.transpose(0, 2).transpose(1, 2)\n",
    "        true_logp = Variable(torch.Tensor(batch.logp)).to(args.device)\n",
    "        pred_logp = model(input_smile)\n",
    "        \n",
    "        train_loss = criterion(pred_logp, true_logp)        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += train_loss.item()\n",
    "        list_train_loss.append({'epoch':batch_idx/len(data_iter)+kwargs['epoch'], 'train_loss':train_loss.item()})\n",
    "        cnt_iter += 1\n",
    "        args.bar.update(len(X))\n",
    "\n",
    "    return model, list_train_loss\n",
    "\n",
    "\n",
    "def validate(model, data_iter, criterion, args):\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        if batch.smile.shape[0] != args.batch_size:\n",
    "            continue\n",
    "\n",
    "        model.eval()\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        input_smile = batch.smile.transpose(0, 2).transpose(1, 2)\n",
    "        true_logp = Variable(torch.Tensor(batch.logp)).to(args.device)\n",
    "        pred_logp = model(input_smile)\n",
    "        \n",
    "        val_loss = criterion(pred_logp, true_logp)        \n",
    "        epoch_val_loss += val_loss.item()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return epoch_val_loss/cnt_iter\n",
    "\n",
    "\n",
    "def test(model, data_iter, args, **kwargs):\n",
    "\n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        \n",
    "        if batch.smile.shape[0] != args.batch_size:\n",
    "            continue\n",
    "\n",
    "        model.eval()\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        input_smile = batch.smile.transpose(0, 2).transpose(1, 2)\n",
    "        true_logp = Variable(torch.Tensor(batch.logp)).to(args.device)\n",
    "        pred_logp = model(input_smile)\n",
    "        \n",
    "        list_y += true_logp.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_logp.cpu().detach().numpy().tolist()\n",
    "        args.bar.update(len(X))\n",
    "\n",
    "    mae = mean_absolute_error(list_y, list_pred_y)\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return mae, std, list_y, list_pred_y\n",
    "\n",
    "def experiment(partition, args):\n",
    "    ts = time.time()\n",
    "\n",
    "    # ===== Construct Model ===== #\n",
    "    model = LSTM(args.vocab_size, args.hidden_dim, batch_size=args.batch_size, \n",
    "                 output_dim=args.output_dim, num_layers=args.n_layer, dropout=0.1,\n",
    "                 bidirectional=args.bidirectional, emb_train=args.emb_train, \n",
    "                 skip_connection=args.skip_connection, cell_type=args.cell_type, norm_type=args.norm_type)    \n",
    "    model.to(args.device)\n",
    "    criterion = nn.MSELoss().to(args.device)\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    overall_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(\"##############################################\")\n",
    "    print(\"Total Model Parameters : {}\".format(trainable_parameters))\n",
    "    print(\"Trainable   Parameters : {}\".format(overall_params))\n",
    "    print(\"##############################################\")\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "    print(\"{} Optimizer is Constructed\".format(str(optimizier)))\n",
    "    print(\"##############################################\")\n",
    "\n",
    "    # Train, Validate, Evaluate\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_mae = list()\n",
    "    list_std = list()\n",
    "    \n",
    "    args.best_mae = 10000\n",
    "    for epoch in range(args.epoch):\n",
    "        model, train_losses = train(model, partition['train'], optimizer, criterion, args, **{'epoch':epoch})\n",
    "        val_loss = validate(model, partition['val'], criterion, args)\n",
    "        mae, std, true_y, pred_y = test(model, partition['val'], args, **{'epoch':epoch})\n",
    "\n",
    "        list_train_loss += train_losses\n",
    "        list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
    "        list_mae.append({'epoch':epoch, 'mae':mae})\n",
    "        list_std.append({'epoch':epoch, 'std':std})\n",
    "        \n",
    "        if args.best_mae > mae or epoch==0:\n",
    "            args.best_epoch = epoch\n",
    "            args.best_mae = mae\n",
    "            args.best_std = std\n",
    "            args.best_true_y = true_y\n",
    "            args.best_pred_y = pred_y\n",
    "    \n",
    "    te = time.time()\n",
    "    \n",
    "    # Logging Experiment Results\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "    args.val_losses = list_val_loss\n",
    "    args.maes = list_mae\n",
    "    args.stds = list_std\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24649 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6401354\n",
      "start 0\n",
      "torch.Size([24, 8, 256])\n",
      "torch.Size([24, 8, 256])\n",
      "torch.Size([24, 8, 256])\n",
      "torch.Size([24, 8, 256])\n",
      "0 2.9893321990966797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "exp_name = 'exp1_test'\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = exp_name\n",
    "args.n_layer = 4\n",
    "args.hidden_dim = 128\n",
    "args.output_dim = 1\n",
    "args.vocab_size = len(CHAR_SMILE.vocab)\n",
    "\n",
    "args.dropout = 0.1\n",
    "args.emb_train = True\n",
    "args.bidirectional = True\n",
    "args.skip_connection = True\n",
    "args.cell_type = 'lstm'\n",
    "args.norm_type = 'bn'\n",
    "\n",
    "args.lr = 0.01\n",
    "args.l2_coef = 0.001\n",
    "args.optim = 'ADAM'\n",
    "args.epoch = 100\n",
    "args.batch_size= 8\n",
    "args.test_batch_size= 8\n",
    "args.shuffle = True\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "writer = Writer(prior_keyword=['n_layer', 'n_stage', 'sc_type', 'use_bn', 'use_attn', 'dp_rate', 'emb_train', 'epoch', 'l2_coef', 'lr'])\n",
    "\n",
    "\n",
    "train_iter, val_iter = BucketIterator.splits(\n",
    "    (train_dataset, val_dataset),\n",
    "    batch_sizes=(args.batch_size, args.test_batch_size),\n",
    "    device='cuda',\n",
    "    sort_key=lambda x: x.length,\n",
    "    sort=True,\n",
    "    sort_within_batch=True,\n",
    "    repeat=False,\n",
    "    shuffle=args.shuffle,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(num_params)\n",
    "for epoch in range(1):\n",
    "    epoch_loss = 0\n",
    "    print('start', epoch)\n",
    "    for batch in tqdm(train_iter):\n",
    "        if batch.smile.shape[0] != args.batch_size:\n",
    "            continue\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizier.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        input_smile = batch.smile.transpose(0, 2).transpose(1, 2)\n",
    "        true_logp = Variable(torch.Tensor(batch.logp)).to(args.device)\n",
    "\n",
    "        pred_logp = model(input_smile)\n",
    "        loss = criterion(pred_logp, true_logp)\n",
    "#         print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizier.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        break\n",
    "\n",
    "    #     break\n",
    "    print(epoch, epoch_loss)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
