{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fae98028110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from utils import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = make_partition(50000, 0.1, 0.1, 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create char_to_ix Ref: https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/word_embeddings_tutorial.py  \n",
    "Pre-defined Embedding Layer Ref: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76  \n",
    "ResNet Variation Ref: https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train, Validate, Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, criterion, args, **kwargs):\n",
    "    data_iter = DataLoader(\n",
    "        partition['train'],\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=args.shuffle\n",
    "    )\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    list_train_loss = list()\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, A, y = batch\n",
    "        X, A, y = X.to(args.device).long(), A.to(args.device).long(), y.to(args.device).float()\n",
    "    \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_y = model(X, A)\n",
    "#         pred_y.require_grad = False\n",
    "        train_loss = criterion(pred_y, y)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        list_train_loss.append({'epoch':batch_idx/len(data_iter)+kwargs['epoch'], 'train_loss':train_loss.item()})\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cnt_iter += 1\n",
    "        args.bar.update(len(X))\n",
    "    return model, list_train_loss\n",
    "\n",
    "def validate(model, partition, criterion, args):\n",
    "    data_iter = DataLoader(\n",
    "        partition['val'],\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=args.shuffle\n",
    "    )\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, A, y = batch\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device).float()\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X, A)\n",
    "        pred_y.require_grad = False\n",
    "        val_loss = criterion(pred_y, y)\n",
    "        epoch_val_loss += val_loss.item()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return epoch_val_loss/cnt_iter\n",
    "\n",
    "def test(model, partition, args, **kwargs):\n",
    "    data_iter = DataLoader(\n",
    "        partition['test'],\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, A, y = batch\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device).float()\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X, A)\n",
    "        list_y += y.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
    "        args.bar.update(len(X))\n",
    "\n",
    "    mae = mean_absolute_error(list_y, list_pred_y)\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return mae, std, list_y, list_pred_y\n",
    "\n",
    "def experiment(partition, args):\n",
    "    ts = time.time()\n",
    "    args.vocab_size = 40\n",
    "    args.max_len = 50\n",
    "    args.input_shape = (args.max_len, args.vocab_size)\n",
    "    model = Net(args)\n",
    "    model.to(args.device)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Train, Validate, Evaluate\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_mae = list()\n",
    "    list_std = list()\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        model, train_losses = train(model, partition, optimizer, criterion, args, **{'epoch':epoch})\n",
    "        val_loss = validate(model, partition, criterion, char_to_ix, args)\n",
    "        mae, std, true_y, pred_y = test(model, partition, args, **{'epoch':epoch})\n",
    "\n",
    "        list_train_loss += train_losses\n",
    "        list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
    "        list_mae.append({'epoch':epoch, 'mae':mae})\n",
    "        list_std.append({'epoch':epoch, 'std':std})\n",
    "        \n",
    "        if args.best_mae > mae or epoch==0:\n",
    "            args.best_epoch = epoch\n",
    "            args.best_mae = mae\n",
    "            args.best_std = std\n",
    "            args.best_true_y = true_y\n",
    "            args.best_pred_y = pred_y\n",
    "    \n",
    "    te = time.time()\n",
    "    \n",
    "    # Logging Experiment Results\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "    args.val_losses = list_val_loss\n",
    "    args.maes = list_mae\n",
    "    args.stds = list_std\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment.1  n_stage vs n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GConv, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        x = self.fc(X)\n",
    "        x = torch.matmul(X, A.long())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_filter, out_filter, stride, use_bn, dp_rate, block_type):\n",
    "        super(ResBlock, self).__init__()   \n",
    "        self.use_bn = use_bn\n",
    "        self.block_type = block_type\n",
    "        self.conv1 = nn.Conv2d(in_filter, out_filter, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(out_filter, out_filter, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filter)\n",
    "        self.bn2 = nn.BatchNorm2d(out_filter)\n",
    "        self.dropout = nn.Dropout2d(dp_rate)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_filter != out_filter:\n",
    "            self.shortcut.add_module(\n",
    "                'conv', nn.Conv2d(in_filter, out_filter,\n",
    "                                  kernel_size=1, stride=stride, \n",
    "                                  padding=0, bias=False)\n",
    "            )\n",
    "        \n",
    "    def forward(self, _x):\n",
    "        if self.block_type == 'a': #original residual block\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.bn2(self.conv2(x)) if self.use_bn else self.conv2(x)\n",
    "            x = x + self.shortcut(_x)\n",
    "            return self.dropout(self.relu(x))\n",
    "        \n",
    "        elif self.block_type == 'b': # BN after addition\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.conv2(x) + self.shortcut(_x)\n",
    "            return self.dropout(self.relu(self.bn2(x)) if self.use_bn else self.relu(x))\n",
    "        \n",
    "        elif self.block_type == 'c': # ReLU before addition\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.relu(self.bn2(self.conv2(x))) if self.use_bn else self.relu(self.conv2(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "        \n",
    "        elif self.block_type == 'd': # ReLU-only pre-activation\n",
    "            x = self.bn1(self.conv1(self.relu(_x))) if self.use_bn else self.conv1(self.relu(_x))\n",
    "            x = self.bn2(self.conv2(self.relu(x))) if self.use_bn else self.conv2(self.relu(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "        \n",
    "        elif self.block_type == 'e': # full pre-activation\n",
    "            x = self.conv1(self.relu(self.bn1(_x))) if self.use_bn else self.conv1(self.relu(_x))\n",
    "            x = self.conv2(self.relu(self.bn2(x))) if self.use_bn else self.conv2(self.relu(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "             \n",
    "            \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Net, self).__init__()   \n",
    "        \n",
    "        # Create Atom Element embedding layer\n",
    "        self.embedding = self.create_emb_layer(args.vocab_size, args.emb_train)\n",
    "        self.fc1 = nn.Linear(args.hidden_dim2, args.hidden_dim2)\n",
    "\n",
    "        \n",
    "        # Create Residual Convolution layer\n",
    "        list_gconvs = list()\n",
    "        \"\"\"\n",
    "        hidden_dim = args.start_hidden_dim\n",
    "        for i in range(args.n_stage):\n",
    "            if i==0:\n",
    "                list_gconvs.append(ResBl(n_channel, n_channel*args.start_channel, args.stride, args.use_bn, args.dp_rate, args.block_type))\n",
    "                n_channel *= args.start_channel\n",
    "            else:\n",
    "                list_res_blocks.append(ResBlock(n_channel, n_channel*2, args.stride, args.use_bn, args.dp_rate, args.block_type))\n",
    "                n_channel *= 2\n",
    "            for j in range(args.n_layer-1):\n",
    "                list_res_blocks.append(ResBlock(n_channel, n_channel, 1, args.use_bn, args.dp_rate, args.block_type))\n",
    "        self.gconvs = nn.Sequential(*list_res_blocks)\n",
    "        \n",
    "        # Create MLP layers\n",
    "#         fc_shape = self._estimate_fc_shape((1, args.max_len,))\n",
    "        self.fc1 = nn.Linear(args.hidden_dim2, args.hidden_dim2)\n",
    "        self.fc2 = nn.Linear(args.hidden_dim2, args.hidden_dim2//2)\n",
    "        self.fc3 = nn.Linear(args.hidden_dim2//2, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        x = self._embed(x)\n",
    "        print('embed', embed.shape)\n",
    "        \"\"\"\n",
    "        x = self._gconv_forward(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.squeeze(x)\n",
    "        \"\"\"\n",
    "        return x\n",
    "    \n",
    "    def _embed(self, x):\n",
    "        embed_x = self.embedding(x[:,:,1])\n",
    "        x = torch.cat((embed_x, x[:,:,1:].float()), 2)\n",
    "        return x \n",
    "        \n",
    "    \n",
    "    def _gconv_forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(embeds.shape[0], 1, embeds.shape[1], embeds.shape[2])\n",
    "        x = self.gconvs(embeds)\n",
    "        return x\n",
    "    \n",
    "    def _estimate_fc_shape(self, input_shape):\n",
    "        dummy_input = torch.zeros(input_shape).long()\n",
    "        dummy_output = self._gconv_forward(dummy_input)\n",
    "        fc_shape = dummy_output.view(dummy_output.shape[0], -1).shape\n",
    "        return fc_shape\n",
    "        \n",
    "\n",
    "    def create_emb_layer(self, vocab_size, emb_train):\n",
    "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "        for i in range(vocab_size):\n",
    "            weight_matrix[i][i] = 1\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "\n",
    "        if not emb_train:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 50])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros((256, 50, 19))\n",
    "b = a[:,:,1]\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([256, 50, 19])\n",
      "A torch.Size([256, 50, 50])\n",
      "embed torch.Size([256, 50, 58])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mse_loss_forward is not implemented for type torch.cuda.LongTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-05a1a9891ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \"\"\"\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Define Hyperparameter Search Space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-c652a9785c86>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(partition, args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-c652a9785c86>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, partition, optimizer, criterion, args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         pred_y.require_grad = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlist_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \"\"\"\n\u001b[1;32m   1568\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[0;32m-> 1569\u001b[0;31m                            input, target, size_average, reduce)\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mse_loss_forward is not implemented for type torch.cuda.LongTensor"
     ]
    }
   ],
   "source": [
    "exp_name = 'exp1_layer_stage'\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = exp_name\n",
    "args.n_layer = 2\n",
    "args.n_stage = 1\n",
    "args.hidden_dim1 = 64\n",
    "args.hidden_dim2 = 256\n",
    "args.lr = 0.00005\n",
    "args.l2_coef = 0.0001\n",
    "args.optim = 'ADAM'\n",
    "args.epoch = 50\n",
    "args.batch_size= 256\n",
    "args.test_batch_size= 256\n",
    "args.emb_train = False\n",
    "args.start_channel = 8\n",
    "args.stride = 2\n",
    "args.use_bn = True\n",
    "args.dp_rate = 0.3\n",
    "args.block_type = 'a'\n",
    "args.max_len= 120\n",
    "args.shuffle = True\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "writer = Writer(prior_keyword=['n_layer', 'n_stage', 'block_type', 'use_bn', 'dp_rate', 'emb_train', 'epoch', 'batch_size'])\n",
    "\n",
    "\n",
    "experiment(partition, args)\n",
    "\"\"\"\n",
    "# Define Hyperparameter Search Space\n",
    "list_n_layer = [1,2,3,4]\n",
    "list_n_stage = [1,2,3,4]\n",
    "\n",
    "# Initialize num iteration, num experiment, progress bar\n",
    "n_iter = args.epoch * (len(partition['train']) + len(partition['test']))\n",
    "n_exp = len(list_n_layer)*len(list_n_stage)\n",
    "cnt_exp = 0\n",
    "bar = tqdm_notebook(total=n_exp*n_iter, file=sys.stdout, position=0)\n",
    "bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "\n",
    "# writer.clear(exp_name)\n",
    "for n_layer in list_n_layer:\n",
    "    for n_stage in list_n_stage:\n",
    "        # Update hyperparameter\n",
    "        args.n_layer = n_layer\n",
    "        args.n_stage = n_stage\n",
    "        args.bar = bar\n",
    "        result = experiment(partition, args)\n",
    "        writer.write(result)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        cnt_exp += 1\n",
    "        bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "        print('[Exp {:2}] got mae: {:2.3f}, std: {:2.3f} at epoch {:2}'.format(cnt_exp, result.best_mae, result.best_std, result.epoch))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = writer.read(exp_name='exp1_layer_stage')\n",
    "results = results.loc[results['epoch']==50]\n",
    "variable1 = 'n_stage'\n",
    "variable2 = 'n_layer'\n",
    "\n",
    "\n",
    "plot_performance(results, variable1, variable2,\n",
    "                'Performance depends on {} vs {}'.format(variable1, variable2),\n",
    "                'exp1_Performance {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plot_distribution(results, variable1, variable2, 'true_y', 'pred_y', \n",
    "                  'Prediction results depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Prediction {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plot_loss(results, variable1, variable2, 'epoch', 'loss', \n",
    "                  'Loss depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Loss {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment.1 n_stage vs n_layer  \n",
    "\n",
    "In this experiment, regression performance was measured by 'mae' metric among the variation of the number of the residual block(n_stage) and the number of the layer per block(n_layer).  \n",
    "\n",
    "**Variable Domain**  \n",
    "- n_stage = [1, 2, 3, 4]  \n",
    "- n_layer = [1, 2, 3, 4]\n",
    "\n",
    "# Results and Discussion\n",
    "\n",
    "1. The first figure shows the mae and std variation depends on the number of residual block and number of convolution layer in the residual block. Also, it notify the experiment settings.  \n",
    "2. The second figure shows the distribution between ground truth y and predicted y with y=x line(dashed).  \n",
    "3. The last figure shows the train loss, validation loss(left y-axis) and mae score (right y-axis).  \n",
    "\n",
    "**Notable Results**  \n",
    "- As the number of the residual block increased, overall performance was increased.  \n",
    "- As the number of the layer in a residual block increased, overall performance was decreased.  \n",
    "- Therefore, 4 block with 1 layer per block achieved highest performance. \n",
    "\n",
    "**Discussion**  \n",
    "- I expected that 4 block with 4 layer per block would outperform other models, however it was not. This results should be reconsidered since the loss chart of 4 block with 4 layer per block shows that the validation loss is still lower than the train loss. Therefore, longer training should be conducted. \n",
    "- As expected, the models with less residual block shows poor prediction performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment.2 Block Type vs Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp2_block_type_batch_norm'\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = exp_name\n",
    "args.n_layer = 1\n",
    "args.n_stage = 4\n",
    "args.lr = 0.00005\n",
    "args.l2_coef = 0.0001\n",
    "args.optim = 'ADAM'\n",
    "args.epoch = 50\n",
    "args.batch_size= 256\n",
    "args.test_batch_size= 256\n",
    "args.emb_train = False\n",
    "args.start_channel = 8\n",
    "args.stride = 2\n",
    "args.dp_rate = 0.3\n",
    "args.max_len= 120\n",
    "args.shuffle = True\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "writer = Writer(prior_keyword=['n_layer', 'n_stage','block_type', 'use_bn', 'dp_rate', 'emb_train', 'epoch', 'batch_size'])\n",
    "partition = list_partition[0]\n",
    "\n",
    "# Define Hyperparameter Search Space\n",
    "list_use_bn = [True, False]\n",
    "list_block_type = ['a', 'b', 'c', 'd']\n",
    "\n",
    "\n",
    "# Initialize num iteration, num experiment, progress bar\n",
    "n_iter = args.epoch * (len(partition['train']) + len(partition['test']))\n",
    "n_exp = len(list_use_bn)*len(list_block_type)\n",
    "cnt_exp = 0\n",
    "bar = tqdm_notebook(total=n_exp*n_iter, file=sys.stdout, position=0)\n",
    "bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "\n",
    "# writer.clear(exp_name)\n",
    "for use_bn in list_use_bn:\n",
    "    for block_type in list_block_type:\n",
    "        # Update hyperparameter\n",
    "        args.use_bn = use_bn\n",
    "        args.block_type = block_type\n",
    "        args.bar = bar\n",
    "        result = experiment(partition, args)\n",
    "        writer.write(result)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        cnt_exp += 1\n",
    "        bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "        print('[Exp {:2}] got mae: {:2.3f}, std: {:2.3f} at epoch {:2}'.format(cnt_exp, result.best_mae, result.best_std, result.epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = writer.read(exp_name='exp2_block_type_batch_norm')\n",
    "results = results.loc[results['epoch']==50]\n",
    "\n",
    "variable1 = 'block_type'\n",
    "variable2 = 'use_bn'\n",
    "\n",
    "\n",
    "plot_performance(results, variable1, variable2,\n",
    "                'Performance depends on {} vs {}'.format(variable1, variable2),\n",
    "                'exp1_Performance {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plot_distribution(results, variable1, variable2, 'true_y', 'pred_y', \n",
    "                  'Prediction results depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Prediction {} vs {}'.format(variable1, variable2), top=0.9)\n",
    "\n",
    "plot_loss(results, variable1, variable2, 'epoch', 'loss', \n",
    "                  'Loss depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Loss {} vs {}'.format(variable1, variable2), top=0.9)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment.2 Block Type vs Batch Normalization\n",
    "\n",
    "In this experiment, regression performance was measured by 'mae' metric among the variation of the type of the residual block(block_type) and the usage of the batch normalization layer in each residual block(use_bn).     \n",
    "\n",
    "**Variable Domain**  \n",
    "- block_type = ['a':'original, 'b':BN after addition, 'c':ReLU before addition, 'd':ReLU-only pre-activation]  \n",
    "- use_bn = [True, False]\n",
    "\n",
    "# Results and Discussion\n",
    "\n",
    "1. The first figure shows the mae and std variation depends on the residual block type and the usage of the batch normalization layer. Also, it notify the experiment settings.  \n",
    "2. The second figure shows the distribution between ground truth y and predicted y with y=x line(dashed).  \n",
    "3. The last figure shows the train loss, validation loss(left y-axis) and mae score (right y-axis).  \n",
    "\n",
    "**Notable Results**  \n",
    "- Among 4 types of residual block, block A and C types are outperformed.\n",
    "- When the batch normalization layer is used, the mae was almost reduced as half. \n",
    "- Also, when the batch normalization is applied, the mae values recorded less variation among different block types.\n",
    "\n",
    "**Discussion**  \n",
    "- The batch normalization layer boosts up the performance more than I expected. (very powerful).  \n",
    "- Different residual block has less significant impact on the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment.3 Trainable Embedding vs Start Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp3_emb_train_start_channel'\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = exp_name\n",
    "args.n_layer = 1\n",
    "args.n_stage = 4\n",
    "args.lr = 0.00005\n",
    "args.l2_coef = 0.0001\n",
    "args.optim = 'ADAM'\n",
    "args.epoch = 50\n",
    "args.batch_size= 256\n",
    "args.test_batch_size= 256\n",
    "args.start_channel = 8\n",
    "args.stride = 2\n",
    "args.dp_rate = 0.3\n",
    "args.max_len= 120\n",
    "args.shuffle = True\n",
    "args.use_bn = True\n",
    "args.block_type = 'a'\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "writer = Writer(prior_keyword=['n_layer', 'n_stage','block_type', 'use_bn', 'dp_rate', 'emb_train', 'epoch', 'start_channel'])\n",
    "partition = list_partition[0]\n",
    "\n",
    "# Define Hyperparameter Search Space\n",
    "list_emb_train = [True, False]\n",
    "list_start_channel = [4,8,16,32]\n",
    "\n",
    "\n",
    "# Initialize num iteration, num experiment, progress bar\n",
    "n_iter = args.epoch * (len(partition['train']) + len(partition['test']))\n",
    "n_exp = len(list_emb_train)*len(list_start_channel)\n",
    "cnt_exp = 0\n",
    "bar = tqdm_notebook(total=n_exp*n_iter, file=sys.stdout, position=0)\n",
    "bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "\n",
    "# writer.clear(exp_name)\n",
    "for emb_train in list_emb_train:\n",
    "    for start_channel in list_start_channel:\n",
    "        # Update hyperparameter\n",
    "        args.emb_train = emb_train\n",
    "        args.start_channel = start_channel\n",
    "        args.bar = bar\n",
    "        \n",
    "        ts = time.time()\n",
    "        result = experiment(partition, args)\n",
    "        writer.write(result)\n",
    "        torch.cuda.empty_cache()\n",
    "        te = time.time()\n",
    "        \n",
    "        cnt_exp += 1\n",
    "        bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "        print('[Exp {:2}] got mae: {:2.3f}, std: {:2.3f} at epoch {:2} Took {:3.1f}'.format(cnt_exp, result.best_mae, result.best_std, result.epoch, te-ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = writer.read(exp_name='exp3_emb_train_start_channel')\n",
    "# results = results.loc[results['epoch']==50]\n",
    "\n",
    "variable1 = 'start_channel'\n",
    "variable2 = 'emb_train'\n",
    "\n",
    "\n",
    "plot_performance(results, variable1, variable2,\n",
    "                'Performance depends on {} vs {}'.format(variable1, variable2),\n",
    "                'exp1_Performance {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plot_distribution(results, variable1, variable2, 'true_y', 'pred_y', \n",
    "                  'Prediction results depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Prediction {} vs {}'.format(variable1, variable2), top=0.9)\n",
    "\n",
    "plot_loss(results, variable1, variable2, 'epoch', 'loss', \n",
    "                  'Loss depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Loss {} vs {}'.format(variable1, variable2), top=0.9)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment.3 Trainable Embedding vs Start Channel\n",
    "\n",
    "In this experiment, regression performance was measured by 'mae' metric among the number of the starting convolution filters(start_channel) and the usage of trainable atom embedding(emb_train).  \n",
    "\n",
    "**Variable Domain**  \n",
    "- emb_train = [True, False]\n",
    "- start_channel = [4,8,16,32,64]\n",
    "\n",
    "# Results and Discussion\n",
    "\n",
    "1. The first figure shows the mae and std variation depends on the number of convolution channel of the first residual block(start_channel) and the trainability of the atom embedding vector(emb_train). Also, it notify the experiment settings.  \n",
    "2. The second figure shows the distribution between ground truth y and predicted y with y=x line(dashed).  \n",
    "3. The last figure shows the train loss, validation loss(left y-axis) and mae score (right y-axis).  \n",
    "\n",
    "**Notable Results**  \n",
    "- As the number of the start channel increased, the performance drastically improved. \n",
    "- When the embedding vectors were trained, the performance was slightly increased. \n",
    "\n",
    "**Discussion**  \n",
    "- Increasing the number of the convolution filter results in the increasing performance. However lots of computation power required.     \n",
    "- Training original embedding vector is meaningful to increase the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp4_lr_l2'\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = exp_name\n",
    "args.n_layer = 1\n",
    "args.n_stage = 4\n",
    "args.emb_train = True\n",
    "args.optim = 'ADAM'\n",
    "args.epoch = 50\n",
    "args.batch_size= 512\n",
    "args.test_batch_size= 512\n",
    "args.start_channel = 16\n",
    "args.stride = 2\n",
    "args.dp_rate = 0.3\n",
    "args.max_len= 120\n",
    "args.shuffle = True\n",
    "args.use_bn = True\n",
    "args.block_type = 'a'\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "writer = Writer(prior_keyword=['n_layer', 'n_stage','block_type', 'use_bn', 'dp_rate', 'emb_train', 'epoch', 'start_channel'])\n",
    "partition = list_partition[0]\n",
    "\n",
    "# Define Hyperparameter Search Space\n",
    "list_lr = [0.00005, 0.0005, 0.005, 0.05]\n",
    "list_l2_coef = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "# Initialize num iteration, num experiment, progress bar\n",
    "n_iter = args.epoch * (len(partition['train']) + len(partition['test']))\n",
    "n_exp = len(list_lr)*len(list_l2_coef)\n",
    "cnt_exp = 0\n",
    "bar = tqdm_notebook(total=n_exp*n_iter, file=sys.stdout, position=0)\n",
    "bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "\n",
    "# writer.clear(exp_name)\n",
    "for lr in list_lr:\n",
    "    for l2_coef in list_l2_coef:\n",
    "        # Update hyperparameter\n",
    "        args.lr = lr\n",
    "        args.l2_coef = l2_coef\n",
    "        args.bar = bar\n",
    "        \n",
    "        ts = time.time()\n",
    "        result = experiment(partition, args)\n",
    "        writer.write(result)\n",
    "        torch.cuda.empty_cache()\n",
    "        te = time.time()\n",
    "        \n",
    "        cnt_exp += 1\n",
    "        bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "        print('[Exp {:2}] got mae: {:2.3f}, std: {:2.3f} at epoch {:2} Took {:3.1f}sec'.format(cnt_exp, result.best_mae, result.best_std, result.epoch, te-ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
