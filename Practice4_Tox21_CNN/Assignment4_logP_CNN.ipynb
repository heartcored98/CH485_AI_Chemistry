{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f21455dcbf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from utils import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.nbits = 2048\n",
    "args.n_splits = 5\n",
    "args.test_size = 0.2\n",
    "args.num_mol = 50000\n",
    "args.max_len = 120\n",
    "args.shuffle = True\n",
    "args.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "args.optim = 'RMSProp'\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_partition = make_partition(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create char_to_ix Ref: https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/word_embeddings_tutorial.py  \n",
    "Pre-defined Embedding Layer Ref: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76  \n",
    "ResNet Variation Ref: https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(filename='../Data/logP/vocab.npy'):\n",
    "    vocab = np.load(filename)\n",
    "    vocab_size = len(vocab)\n",
    "    char_to_ix = {char: i for i, char in enumerate(vocab)}\n",
    "    return vocab, char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_filter, out_filter, stride, use_bn, dp_rate, block_type):\n",
    "        super(ResBlock, self).__init__()   \n",
    "        self.use_bn = use_bn\n",
    "        self.block_type = block_type\n",
    "        self.conv1 = nn.Conv2d(in_filter, out_filter, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(out_filter, out_filter, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filter)\n",
    "        self.bn2 = nn.BatchNorm2d(out_filter)\n",
    "        self.dropout = nn.Dropout2d(dp_rate)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_filter != out_filter:\n",
    "            self.shortcut.add_module(\n",
    "                'conv', nn.Conv2d(in_filter, out_filter,\n",
    "                                  kernel_size=1, stride=stride, \n",
    "                                  padding=0, bias=False)\n",
    "            )\n",
    "        \n",
    "    def forward(self, _x):\n",
    "        if self.block_type == 'a': #original residual block\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.bn2(self.conv2(x)) if self.use_bn else self.conv2(x)\n",
    "            x = x + self.shortcut(_x)\n",
    "            return self.dropout(self.relu(x))\n",
    "        \n",
    "        elif self.block_type == 'b': # BN after addition\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.conv2(x) + self.shortcut(_x)\n",
    "            return self.dropout(self.relu(self.bn2(x)) if self.use_bn else self.relu(x))\n",
    "        \n",
    "        elif self.block_type == 'c': # ReLU before addition\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.relu(self.bn2(self.conv2(x))) if self.use_bn else self.relu(self.conv2(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "        \n",
    "        elif self.block_type == 'd': # ReLU-only pre-activation\n",
    "            x = self.bn1(self.conv1(self.relu(_x))) if self.use_bn else self.conv1(self.relu(_x))\n",
    "            x = self.bn2(self.conv2(self.relu(x))) if self.use_bn else self.conv2(self.relu(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "        \n",
    "        elif self.block_type == 'e': # full pre-activation\n",
    "            x = self.conv1(self.relu(self.bn1(_x))) if self.use_bn else self.conv1(self.relu(_x))\n",
    "            x = self.conv2(self.relu(self.bn2(x))) if self.use_bn else self.conv2(self.relu(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "             \n",
    "            \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Net, self).__init__()   \n",
    "        list_n_filter = [32*2**i for i in range(args.n_layer)]\n",
    "        \n",
    "        self.embedding = self.create_emb_layer(args.vocab_size, args.emb_train)\n",
    "        list_res_blocks = [\n",
    "            ResBlock(1, 32, args.stride, args.use_bn, args.dp_rate, args.block_type)\n",
    "        ]\n",
    "        self.res_blocks = nn.Sequential(*list_res_blocks)\n",
    "        self.fc = nn.Linear(122880, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(embeds.shape[0], 1, embeds.shape[1], embeds.shape[2])\n",
    "        x = self.res_blocks(embeds)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "    def create_emb_layer(self, vocab_size, emb_train):\n",
    "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "        for i in range(vocab_size):\n",
    "            weight_matrix[i][i] = 1\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "\n",
    "        if not emb_train:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, criterion, char_to_ix, args):\n",
    "    data_iter = DataLoader(\n",
    "        partition['train'],\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=args.shuffle\n",
    "    )\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, y = batch[0], batch[1]\n",
    "        X = torch.Tensor([[char_to_ix[c] for c in smile] for smile in X]).long()\n",
    "        X, y = X.to(args.device), y.to(args.device).float()\n",
    "    \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_y = model(X)\n",
    "        pred_y.require_grad = False\n",
    "        train_loss = criterion(pred_y, y)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return model, epoch_train_loss/cnt_iter\n",
    "\n",
    "def validate(model, partition, criterion, char_to_ix, args):\n",
    "    data_iter = DataLoader(\n",
    "        partition['val'],\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=args.shuffle\n",
    "    )\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, y = batch[0], batch[1]\n",
    "        X = torch.Tensor([[char_to_ix[c] for c in smile] for smile in X]).long()\n",
    "        X, y = X.to(args.device), y.to(args.device).float()\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X)\n",
    "        pred_y.require_grad = False\n",
    "        val_loss = criterion(pred_y, y)\n",
    "        epoch_val_loss += val_loss.item()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return epoch_val_loss/cnt_iter\n",
    "\n",
    "def test(model, partition, char_to_ix, args):\n",
    "    data_iter = DataLoader(\n",
    "        partition['test'],\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=args.shuffle\n",
    "    )\n",
    "    \n",
    "    cnt_iter = 0\n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, y = batch[0], batch[1]\n",
    "        X = torch.Tensor([[char_to_ix[c] for c in smile] for smile in X]).long()\n",
    "        X, y = X.to(args.device), y.to(args.device).float()\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X)\n",
    "        list_y += y.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    mae = mean_absolute_error(list_y, list_pred_y)\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return mae, std\n",
    "\n",
    "def experiment(partition, args):\n",
    "    vocab, char_to_ix = create_vocab()\n",
    "    args.vocab_size = len(vocab)\n",
    "    args.input_shape = (args.max_len, args.vocab_size)\n",
    "    model = Net(args)\n",
    "    model.to(args.device)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    for epoch in range(args.epoch):\n",
    "        model, train_loss = train(model, partition, optimizer, criterion, char_to_ix, args)\n",
    "        val_loss = validate(model, partition, criterion, char_to_ix, args)\n",
    "        mae, std = test(model, partition, char_to_ix, args)\n",
    "        \n",
    "        print(train_loss, val_loss, mae, std)\n",
    "        \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165.41904091644287 3.1543420553207397 1.5763102052140865 0.8701013226664565\n"
     ]
    }
   ],
   "source": [
    "args.exp_name = 'exp_hidden_dim'\n",
    "args.n_layer = 2\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0\n",
    "args.optim = 'RMSProp'\n",
    "args.epoch = 1\n",
    "args.batch_size=256\n",
    "args.test_batch_size=1024\n",
    "args.emb_train = False\n",
    "args.stride = 1\n",
    "args.use_bn = True\n",
    "args.dp_rate = 0\n",
    "args.block_type = 'a'\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "experiment(list_partition[0], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
