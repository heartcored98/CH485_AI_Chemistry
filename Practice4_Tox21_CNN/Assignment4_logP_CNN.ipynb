{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f180f3cb090>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from utils import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.nbits = 2048\n",
    "args.n_splits = 5\n",
    "args.test_size = 0.2\n",
    "args.num_mol = 50000\n",
    "args.max_len = 120\n",
    "args.shuffle = True\n",
    "args.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "args.optim = 'RMSProp'\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_partition = make_partition(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create char_to_ix Ref: https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/word_embeddings_tutorial.py  \n",
    "Pre-defined Embedding Layer Ref: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76  \n",
    "ResNet Variation Ref: https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(filename='../Data/logP/vocab.npy'):\n",
    "    vocab = np.load(filename)\n",
    "    vocab_size = len(vocab)\n",
    "    char_to_ix = {char: i for i, char in enumerate(vocab)}\n",
    "    return vocab, char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_filter, out_filter, stride, use_bn, dp_rate, block_type):\n",
    "        super(ResBlock, self).__init__()   \n",
    "        self.use_bn = use_bn\n",
    "        self.block_type = block_type\n",
    "        self.conv1 = nn.Conv2d(in_filter, out_filter, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(out_filter, out_filter, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filter)\n",
    "        self.bn2 = nn.BatchNorm2d(out_filter)\n",
    "        self.dropout = nn.Dropout2d(dp_rate)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_filter != out_filter:\n",
    "            self.shortcut.add_module(\n",
    "                'conv', nn.Conv2d(in_filter, out_filter,\n",
    "                                  kernel_size=1, stride=stride, \n",
    "                                  padding=0, bias=False)\n",
    "            )\n",
    "        \n",
    "    def forward(self, _x):\n",
    "        if self.block_type == 'a': #original residual block\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.bn2(self.conv2(x)) if self.use_bn else self.conv2(x)\n",
    "            x = x + self.shortcut(_x)\n",
    "            return self.dropout(self.relu(x))\n",
    "        \n",
    "        elif self.block_type == 'b': # BN after addition\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.conv2(x) + self.shortcut(_x)\n",
    "            return self.dropout(self.relu(self.bn2(x)) if self.use_bn else self.relu(x))\n",
    "        \n",
    "        elif self.block_type == 'c': # ReLU before addition\n",
    "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
    "            x = self.relu(self.bn2(self.conv2(x))) if self.use_bn else self.relu(self.conv2(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "        \n",
    "        elif self.block_type == 'd': # ReLU-only pre-activation\n",
    "            x = self.bn1(self.conv1(self.relu(_x))) if self.use_bn else self.conv1(self.relu(_x))\n",
    "            x = self.bn2(self.conv2(self.relu(x))) if self.use_bn else self.conv2(self.relu(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "        \n",
    "        elif self.block_type == 'e': # full pre-activation\n",
    "            x = self.conv1(self.relu(self.bn1(_x))) if self.use_bn else self.conv1(self.relu(_x))\n",
    "            x = self.conv2(self.relu(self.bn2(x))) if self.use_bn else self.conv2(self.relu(x))\n",
    "            return self.dropout(x + self.shortcut(_x))\n",
    "             \n",
    "            \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Net, self).__init__()   \n",
    "        \n",
    "        # Create Atom Element embedding layer\n",
    "        self.embedding = self.create_emb_layer(args.vocab_size, args.emb_train)\n",
    "        \n",
    "        # Create Residual Convolution layer\n",
    "        list_res_blocks = list()\n",
    "        n_channel = 1\n",
    "        for i in range(args.n_stage):\n",
    "            if i==0:\n",
    "                list_res_blocks.append(ResBlock(n_channel, n_channel*args.start_channel, args.stride, args.use_bn, args.dp_rate, args.block_type))\n",
    "                n_channel *= args.start_channel\n",
    "            else:\n",
    "                list_res_blocks.append(ResBlock(n_channel, n_channel*2, args.stride, args.use_bn, args.dp_rate, args.block_type))\n",
    "                n_channel *= 2\n",
    "            for j in range(args.n_layer-1):\n",
    "                list_res_blocks.append(ResBlock(n_channel, n_channel, 1, args.use_bn, args.dp_rate, args.block_type))\n",
    "        self.res_blocks = nn.Sequential(*list_res_blocks)\n",
    "        \n",
    "        # Create MLP layers\n",
    "        fc_shape = self._estimate_fc_shape((1, args.max_len,))\n",
    "#         fc_shape = (4000, 200)\n",
    "        self.fc1 = nn.Linear(fc_shape[-1], 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._conv_forward(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def _conv_forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(embeds.shape[0], 1, embeds.shape[1], embeds.shape[2])\n",
    "        x = self.res_blocks(embeds)\n",
    "        return x\n",
    "    \n",
    "    def _estimate_fc_shape(self, input_shape):\n",
    "        dummy_input = torch.zeros(input_shape).long()\n",
    "        dummy_output = self._conv_forward(dummy_input)\n",
    "        fc_shape = dummy_output.view(dummy_output.shape[0], -1).shape\n",
    "        return fc_shape\n",
    "        \n",
    "\n",
    "    def create_emb_layer(self, vocab_size, emb_train):\n",
    "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "        for i in range(vocab_size):\n",
    "            weight_matrix[i][i] = 1\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "\n",
    "        if not emb_train:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train, Validate, Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, criterion, char_to_ix, args, **kwargs):\n",
    "    data_iter = DataLoader(\n",
    "        partition['train'],\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=args.shuffle\n",
    "    )\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, y = batch[0], batch[1]\n",
    "        X = torch.Tensor([[char_to_ix[c] for c in smile] for smile in X]).long()\n",
    "        X, y = X.to(args.device), y.to(args.device).float()\n",
    "    \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_y = model(X)\n",
    "        pred_y.require_grad = False\n",
    "        train_loss = criterion(pred_y, y)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cnt_iter += 1\n",
    "        args.bar.update(len(X))\n",
    "\n",
    "    return model, epoch_train_loss/cnt_iter\n",
    "\n",
    "def validate(model, partition, criterion, char_to_ix, args):\n",
    "    data_iter = DataLoader(\n",
    "        partition['val'],\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=args.shuffle\n",
    "    )\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, y = batch[0], batch[1]\n",
    "        X = torch.Tensor([[char_to_ix[c] for c in smile] for smile in X]).long()\n",
    "        X, y = X.to(args.device), y.to(args.device).float()\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X)\n",
    "        pred_y.require_grad = False\n",
    "        val_loss = criterion(pred_y, y)\n",
    "        epoch_val_loss += val_loss.item()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return epoch_val_loss/cnt_iter\n",
    "\n",
    "def test(model, partition, char_to_ix, args, **kwargs):\n",
    "    data_iter = DataLoader(\n",
    "        partition['test'],\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(data_iter):\n",
    "        X, y = batch[0], batch[1]\n",
    "        X = torch.Tensor([[char_to_ix[c] for c in smile] for smile in X]).long()\n",
    "        X, y = X.to(args.device), y.to(args.device).float()\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X)\n",
    "        list_y += y.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
    "        args.bar.update(len(X))\n",
    "\n",
    "    mae = mean_absolute_error(list_y, list_pred_y)\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return mae, std, np.array(list_y), np.array(list_pred_y)\n",
    "\n",
    "def experiment(partition, args):\n",
    "    ts = time.time()\n",
    "    vocab, char_to_ix = create_vocab()\n",
    "    args.vocab_size = len(vocab)\n",
    "    args.input_shape = (args.max_len, args.vocab_size)\n",
    "    model = Net(args)\n",
    "    model.to(args.device)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Train, Validate, Evaluate\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_mae = list()\n",
    "    list_std = list()\n",
    "    \n",
    "    args.best_mae = 10000\n",
    "    for epoch in range(args.epoch):\n",
    "        model, train_loss = train(model, partition, optimizer, criterion, char_to_ix, args, **{'bar':bar})\n",
    "        val_loss = validate(model, partition, criterion, char_to_ix, args)\n",
    "        mae, std, true_y, pred_y = test(model, partition, char_to_ix, args, **{'bar':bar})\n",
    "\n",
    "        list_train_loss.append(train_loss)\n",
    "        list_val_loss.append(val_loss)\n",
    "        list_mae.append(mae)\n",
    "        list_std.append(std)\n",
    "        \n",
    "        if args.best_mae > mae:\n",
    "            args.best_epoch = epoch\n",
    "            args.best_mae = mae\n",
    "            args.best_std = std\n",
    "            args.best_true_y = true_y\n",
    "            args.best_pred_y = pred_y\n",
    "    \n",
    "    te = time.time()\n",
    "    \n",
    "    # Logging Experiment Results\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "    args.val_losses = list_val_loss\n",
    "    args.maes = list_mae\n",
    "    args.stds = list_std\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5a355c572445d581fc3b64274f29e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=504000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Exp  1] got mae: 1.33, std: 1.30 at epoch  3\n",
      "[Exp  2] got mae: 1.33, std: 1.30 at epoch  3\n",
      "[Exp  3] got mae: 1.33, std: 1.30 at epoch  3\n",
      "[Exp  4] got mae: 1.33, std: 1.30 at epoch  3\n"
     ]
    }
   ],
   "source": [
    "args.exp_name = 'exp_hidden_dim'\n",
    "args.n_layer = 2\n",
    "args.n_stage = 1\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0\n",
    "args.optim = 'RMSProp'\n",
    "args.epoch = 3\n",
    "args.batch_size=32\n",
    "args.test_batch_size=32\n",
    "args.emb_train = False\n",
    "args.start_channel = 8\n",
    "args.stride = 1\n",
    "args.use_bn = True\n",
    "args.dp_rate = 0\n",
    "args.block_type = 'a'\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "writer = Writer(prior_keyword=['n_layer', 'block_type', 'use_bn', 'dp_rate', 'emb_train', 'epoch', 'batch_size'])\n",
    "partition = list_partition[0]\n",
    "\n",
    "# Define Hyperparameter Search Space\n",
    "list_n_layer = [1, 2]#,3,4,5]\n",
    "list_n_stage = [1, 2]#,3,4,5]\n",
    "\n",
    "# Initialize num iteration, num experiment, progress bar\n",
    "n_iter = args.epoch * (len(partition['train']) + len(partition['test']))\n",
    "n_exp = len(list_n_layer)*len(list_n_stage)\n",
    "cnt_exp = 0\n",
    "bar = tqdm_notebook(total=n_exp*n_iter, file=sys.stdout, position=0)\n",
    "bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "\n",
    "for n_layer in list_n_layer:\n",
    "    for n_stage in list_n_stage:\n",
    "        # Update hyperparameter\n",
    "        args.n_layer = n_layer\n",
    "        args.n_stage = n_stage\n",
    "        args.bar = bar\n",
    "        result = experiment(partition, args)\n",
    "        writer.write(result)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        cnt_exp += 1\n",
    "        bar.set_description('P {:2}/{} Exp'.format(cnt_exp, n_exp))\n",
    "        print('[Exp {:2}] got mae: {:2.2f}, std: {:2.2f} at epoch {:2}'.format(cnt_exp, result.best_mae, result.best_std, result.epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
